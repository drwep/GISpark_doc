{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 分布式计算与持续交付\n",
    "\n",
    "大数据技术需要形成生产力，一个强大的持续集成和持续交付的软件工程环境和收缩性强的支持大规模分布式计算的基础设施是必不可少的。其中的软件是大数据分析项目中的重要组成部分，一方面提供硬件设施之上的基础运行环境，另一方面，软件又是数据发挥价值的“大脑”。\n",
    "\n",
    "这里介绍将多种开源工具和在线服务组合形成持续交付系统实现分布式计算的架构和方法。GISpark集成了软件工程持续集成环境的系列工具，包括代码管理、代码审查、模块仓库、系统构建、即时交付、在线文档和Anaconda科学计算环境、Jupyter Notebook交互计算WebUI系统以及Spark和Hadoop大数据处理平台。使用GISpark的软件工具集，可以实现从代码到组件、集群化部署和分布式运行的全生命期过程，提供强大的智能计算能力，动态地、自动化地产出有效的分析结果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 持续集成与持续交付\n",
    "\n",
    "持续集成与持续交付系统包括源代码版本管理、代码审查系统、软件仓库系统、持续集成系统、自动化测试与Bug跟踪、在线文档持续交付以及服务总线等子系统。\n",
    "\n",
    "### Git版本管理系统\n",
    "\n",
    "[Git](https://git-scm.com/)是一个开源的分布式的版本管理系统，由Linux的创始人Linus发起和实现，现在由[开源社区](https://github.com/git)进行发展。与传统的版本管理不同的是，Git中是以开发者为中心而不是以管理者为中心的，每一个参与者都可以拥有一份完整的代码版本，可以进行离线编辑、在线提交、多分支合并等操作，适合用于支持大型的、多分支、多个节点的分布式软件开发模式，不仅可用于管理源代码，也可以用于管理版本化文档和小型的测试数据集等。\n",
    "\n",
    "[Git](https://git-scm.com/)已经成为开源软件版本管理的事实标准，Linux的内核即采用Git进行管理，支持基于互联网的全球范围的多级软件协作。基于Git的在线代码管理服务包括[Github.com](Github.com)和[git.oschina.net](git.oschina.net)等，也可以使用[GitLab]((https://about.gitlab.com/))搭建自己专属的代码托管服务系统。通过将本地库、局域仓库和在线的代码库结合起来，开发者可以与全球各地的同行紧密协作，创建出超大规模的软件系统。\n",
    "\n",
    "### GitLab软件代码托管\n",
    "\n",
    "[Github](http://www.github.com)是全球开源软件开发者的宝库，可以在线地获取各种功能强大的软件的源代码，加以修改并提交更新，使开源软件得以持续不断地改进。不仅得到大量的软件爱好者的拥趸，而且也得到了大量的知名软件企业的大力支持。\n",
    "\n",
    "[GitLab源代码托管与版本管理系统](https://about.gitlab.com/)提供与Github类似的强大功能。但不同的是，GitLab让使用者可以搭建自己的源代码仓库服务系统，可通过Web界面进行访问公开的或者私人项目。能够浏览源代码，管理缺陷和注释。可以管理团队对仓库的访问，非常易于浏览提交过的版本并提供一个文件历史库。它还提供一个代码片段收集功能可以轻松实现代码复用，便于日后有需要的时候进行查找。\n",
    "\n",
    "GitLab的客户端仍然使用Git进行操作，可以与Github等各种服务进行代码和文档共享。GitLab基于工程(Project)的源代码和相关资源的版本化管理Web服务站点，底层采用[Git](https://git-scm.com/)，可以实现分布式的代码编辑和合并，在各种内部团队和企业软件开发中得到越来越多的使用。\n",
    "\n",
    "### Sonar与代码审查\n",
    "\n",
    "在编制大型软件的时候，需要大量的开发者参与，由于不同的软件开发者的知识、经验、习惯等方面的不同，代码往往出现大量的不一致、不规范的情况，从而导致缺陷和维护的困难，从而引发质量问题和严重后果。因此，代码审查(Code Review)和代码的相互评审是提高代码质量的重要手段（由于代码的关联性，到了测试阶段再去修复的时候往往已经为时过晚）。\n",
    "\n",
    "传统的代码审查通过专家评审和相互评审方式进行，工作量大、成本很高。[ReviewBoard](https://www.reviewboard.org/)可以帮助交互式地进行代码审查，提高代码审查的效率。它提供了在diffs里进行语法彩色编码，使得代码阅读变得简便（使用参考[ReviewBoard代码评审实践总结](http://my.oschina.net/u/2306127/blog/541250)）。此外，它还实现了基于Lucene的搜索来帮助管理较大的diffs组。Review Board在审查补丁（Patch）方面表现完美。一个叫做“提交审查”的工具被用来和SCM系统进行连接（当前支持SVN、CVS、Perforce、Git和Mercurial等），可以允许你请求一个将被提交的修改的审查。用户基础页面将这个过程描述如下：\n",
    "\n",
    "1.    你在本地检出的代码上做了些可怕的修改。\n",
    "*    你通过公布diff、编写描述和选择一些审查者来创建一个审查请求。\n",
    "*    你在审查请求中点击“发布”并等待你的审查者看到它。\n",
    "*    其他人看了你的审查请求，说道“这太可怕了，除非什么东西坏掉了。”\n",
    "*    你根据他们的评论更新了你的代码。\n",
    "*    你公布了更新后的diff，以及对他们评论的解答以指明你修改了什么（或者你要说明为什么你不打算按照他们的建议修改代码）。\n",
    "*    大家看了你更新后的代码，并请你继续。\n",
    "*    你将修改提交到仓库中。\n",
    "*    你在审查请求中点击“设置为已提交”来从其他人的面板中移除你的审查请求。\n",
    "\n",
    "[Sonar](https://github.com/SonarSource)是一个自动化的代码审查工具，可以帮助改善源代码的质量，内置了大量的语法、格式等错误检查的规则。[开源中国](http://git.oschina.net)集成了[Sonar在线服务系统](http://sonar.oschina.net/)，其上托管的的项目可以进行代码质量的在线检查。Sonar通过预定义的编码规则，检查常见的编码错误和促进源代码编写的一致性，形成代码质量的数据报告。Sonar支持的语言包括：Java、PHP、C#、C、Cobol、PL/SQL、Flex 等。主要特点包括：(1)代码覆盖，通过单元测试，将会显示哪行代码被选中；（2）改善编码规则；（3）搜寻编码规则，按照名字，插件，激活级别和类别进行查询；（4）项目搜寻，按照项目的名字进行查询；（5）对比数据，比较同一张表中的任何测量的趋势。\n",
    "\n",
    "### Maven软件仓库\n",
    "[Maven软件仓库](http://maven.apache.org/)管理软件构建(Build)中用到的组件，可以集中管理、按需获取。Maven提供一个中心化的软件仓库服务，也可以搭建自己的私有软件仓库。Maven基于project object model (POM)的工程依赖模块定义，可以自动地获取需要的模块进行软件的自动化构建而无需预先安装。由于Maven中心仓库访问量大、速度较慢，可以使用 [Sonatype Nexus](http://www.sonatype.org) 搭建私有组件仓库和中心仓库的本地镜像，参考[安装方法](http://my.oschina.net/MrMichael/blog/295979)。\n",
    "\n",
    "### Jenkins持续集成\n",
    "\n",
    "随着软件开发复杂度的不断提高，团队开发成员间如何更好地协同工作以确保软件开发的质量已经慢慢成为开发过程中不可回避的问题。尤其是近些年来，敏捷（Agile） 在软件工程领域越来越红火，如何能再不断变化的需求中快速适应和保证软件的质量也显得尤其的重要。持续集成正是针对这一类问题的一种软件开发实践。它倡导团队开发成员必须经常集成他们的工作，甚至每天都可能发生多次集成。而每次的集成都是通过自动化的构建来验证，包括自动编译、发布和测试，从而尽快地发现集成错误，让团队能够更快的开发高质量的软件。\n",
    "\n",
    "[Jenkins持续构建系统](https://jenkins.io/index.html)将代码构建、持续集成与持续交付集成为一个整合性服务，实现从源码到运行的完整过程。最新的版本的Jenkins可以实现DevOps(开发运维一体化)的自动化部署，从而实现持续交付。采用Jenkins实现将分布式系统的组件部署到多个节点之中，这一过程可以通过虚拟机和Docker来完成，Jenkins的代码构建结果将被输出到Docker的构建环节，通过Dockerfile实现以Docker容器形式的最终交付件，并继续调用Docker Compose将一组服务立即投入运行。详细使用方法参考[基于 Jenkins 快速搭建持续集成环境](http://www.ibm.com/developerworks/cn/java/j-lo-jenkins/)。\n",
    "\n",
    "### 自动化测试与Bug跟踪\n",
    "\n",
    "交付软件是容易的，交付高质量的产品和服务是非常困难的。建立自动化测试平台、测试用例库和Bug跟踪体系，通过自动化测试流程将软件交付过程形成闭环，并度量和跟踪其中的改变，进行变更控制，从而持续提升交付成果的质量。由于测试的设备和语言、环境的多样性，需要根据需求选择测试软件，可以[参考这里](http://www.oschina.net/search?scope=project&q=%E8%87%AA%E5%8A%A8%E5%8C%96%E6%B5%8B%E8%AF%95)。\n",
    "\n",
    "### 文档的持续交付\n",
    "\n",
    "软件具备清晰、详细的文档对于使用者和软件的升级维护都具有重要的意义，但是文档的编写需要消耗巨大的工作量。随着软件的修改，原来的文档可能很快不再适用，是软件开发中存在的普遍问题。基于[Git](https://git-scm.com/) + [Sphinx](http://www.sphinx-doc.org) + [ReadTheDocs](http://www.ReadTheDocs.org)的工具组合，可以形成一套版本化的在线文档发布系统，简化文档更新的步骤，加快最新版本文档发布的过程。\n",
    "\n",
    "Git可以提供版本化的文档管理，支持多人同时协作，可以采用[Markdown](http://my.oschina.net/u/2306127/blog/655706)(*.md)格式或者IPython Notebook的*.ipynb格式。\n",
    "\n",
    "* 可以将\\*.md文件组装为[Gitbook](https://www.gitbook.com/)的文档系统。\n",
    "* 通过[NBviewer](http://nbviewer.jupyter.org/)进行Notebook文档预览。或者，\n",
    "* 通过Sphinx的[NBSphinx](http://nbsphinx.readthedocs.io)插件将ipynb动态地转换为rst文档，并组装到Sphinx文档系统。\n",
    "* 然后通过[ReadTheDocs](http://www.ReadTheDocs.org)在线发布出来。\n",
    "\n",
    "[Sphinx](http://www.sphinx-doc.org)是一个文档组装系统([中文使用手册](http://zh-sphinx-doc.readthedocs.io/en/latest/tutorial.html))，可以将多种类型文档(rst,md,ipynb...)组装为一个文档，并自动产生索引。Sphinx默认使用的格式是rst ([reStructuredText](http://docutils.sourceforge.net/rst.html))，采用类似于[Markdown](http://my.oschina.net/u/2306127/blog/655706)的格式，但支持更多的精细排版操作。通过扩展插件，可以支持更多的格式，并输出为Latex/PDF等电子文档格式，适合多种设备上的阅读。Sphinx支持多种风格模版，最初用于Python开发的自动化文档生成，其本身亦采用Python开发，非常容易通过插件扩展其功能，或者通过配置文件调整文档生成的各种参数。\n",
    "\n",
    "[ReadTheDocs](http://www.ReadTheDocs.org)是一个免费的在线文档发布系统，可以将Sphinx的文档项目直接发布到“文档云”上。大量的软件使用手册已经采用该平台发布（如[Jupyter](http://jupyter.readthedocs.io/en/latest/)）。只需要注册一个账号，使用Sphinx编写文档项目，并提交到Github，再将[ReadTheDocs](http://www.ReadTheDocs.org)的项目与[Github](http://github.com)账号和Git库连接，系统即可自动获取文档内容、编译并产生最终的HTML发布文档。发布的文档将自动给出\\*.readthedocs.io的域名，可以在浏览器中直接访问，或者将其重定向到自己的域名。如果编译中需要安装其它组件，在根目录下创建一个requirement.txt文件，将组件名称写入再提交即可，与Python的安装模式是完全一样的。\n",
    "\n",
    "### 企业服务总线\n",
    "\n",
    "随着信息化程度的提升，组织机构中的各种服务将会快速增加，需要进行树立和统一管理，企业服务总线就应运而生。各种资源和服务需要连接起来，方便查找、使用和维护，同时可以用于负载均衡、故障转移、失效恢复等工作。\n",
    "\n",
    "企业服务总线主要有三种技术形式，[SOAP](http://www.bing.com/knows/search?q=%E7%AE%80%E5%8D%95%E5%AF%B9%E8%B1%A1%E8%AE%BF%E9%97%AE%E5%8D%8F%E8%AE%AE&mkt=zh-cn&FORM=BKACAI)、[REST](http://www.bing.com/knows/search?q=rest&mkt=zh-cn&FORM=BKACAI)和消息总线。基于SOAP的服务总线主要用于传统的服务集成，正在被基于REST的服务架构所取代，很多企业服务总线软件可以同时这两种协议，这两种协议主要用于中心式的集中管理架构。\n",
    "\n",
    "基于消息总线的服务总线提供对等的体系架构，非常适合松散耦合的大规模、分布式系统，如物联网、社交软件中数据流处理以及多种软件之间分布式的即时协同等等，近年来得到快速发展，正在逐步取代传统的中心式架构。消息协议主要有已经成为国际标准的[AMQP](http://www.amqp.org/)/[MQTT](http://mqtt.org/)，以及JMS或者自定义的格式(如，[coAP](http://blog.csdn.net/xukai871105/article/details/17734163)是基于UDP的轻量级物联网协议，不过目前使用还不够广泛)。消息服务器可以采用[RabbitMQ](http://rabbitmq.org)/[ActiveMQ](http://activemq.apache.org)/[Mosquitto](http://mosquitto.org/)等，RabbitMQ可以同时支持AMQP和MQTT协议。[Paho](http://www.eclipse.org/paho/)是基于MQTT的客户端，支持多种开发语言和小型设备通过MQTT协议进行互联。\n",
    "\n",
    "目前Amazon/Aliyun等云服务商都推出了基于云的消息总线服务，可以支持多种消息协议，只需要申请一个账号即可立即开通服务，可以在整个互联网上使用。大多数时候，需要将云服务和局域网服务结合起来。原则上，本地服务应通过网关连接云服务，从而避免本地端口暴露在互联网上带来的安全性问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anaconda科学计算软件包\n",
    "[Anaconda](https://www.continuum.io/)是一个集成了[Jupyter Notebook](http://jupyter.org/)以及[NumPy](http://numpy.readthedocs.io/en/latest/)、[SciPy](http://scipy.org/)、[Pandas](http://pandas.pydata.org/)、[Matplotlib](http://matplotlib.org/)等科学计算与数据处理的[Python](http://www.python.org)分发包（[使用参考](http://docs.anaconda.org/using.html)）。[continuum公司](https://www.continuum.io/)还开发了企业版，可以让研究人员共享数据和运行环境，从而实现基于网络的团队协同分析。\n",
    "\n",
    "Anaconda采用[conda](http://conda.readthedocs.io/en/latest/)进行软件包的管理，可以实现安装、列表、搜索等各种功能，也可以在[Anaconda Cloud](http://anaconda.org)中创建自己的软件包。Conda的功能类似于pip和Virtualenv的组合，可以创建和管理运行的虚拟环境从而避免不同版本引起的冲突问题（安装在Anaconda3/envs目录下），conda也提供了完整的安装和卸载运行组件的功能，参考[conda的使用](http://my.oschina.net/u/2306127/blog/634674)，同时Anaconda也支持通过pip进行软件组件的安装。\n",
    "\n",
    "Anaconda支持[Spark分布式计算环境](http://spark.apache.org)和[TensorFlow机器学习引擎](http://tensorflow.org/)，参考[Anaconda上安装TensorFlow/Spark，实现jupyter远程访问](http://my.oschina.net/u/2306127/blog/636872)以及[使用Anaconda集成IPython、Spark和TensorFlow、Orange](http://my.oschina.net/u/2306127/blog/636289)。[Orange](http://orange.biolab.si/)是支持可视化流程设计的多算法机器学习计算框架，可以通过Conda中安装和使用，其核心算法也可以在Jupyter Notebook中调用。\n",
    "\n",
    "[Jupyter Notebook](http://jupyter.org/)是Anaconda的重要组件，源于[IPython Notebook]()项目。Jupyter可以创建出“可运行”的Web文档，非常便于Python算法学习和研究交流使用，已被大量的科研、数据分析、金融分析人员使用，并发展出了大量的增强工具。在最新的Jupyter版本中，不仅可以运行python和R代码，还可以通过Magics操作符（%和%%）同时运行shell、ruby等代码。Jupyter Notebook文档可以被转换为HTML/Latex/PDF等格式，在多种设备上浏览。\n",
    "\n",
    "创建Jupyter NoteBook Server的步骤如下：\n",
    "\n",
    "```\n",
    "#创建jupyter配置文件，原来的IPython的一些教程要退休了，按这个：\n",
    "jupyter notebook --generate-config\n",
    "\n",
    "#修改jupyter配置文件，在~/.jupyter/下。\n",
    "gedit ~/.jupyter/jupyter_notebook_config.py\n",
    "\n",
    "#设置访问密码。\n",
    "#需要使用加密的字符串，参考jupyter_notebook_config.py文件中的说明。\n",
    "\n",
    "#设置服务器的IP地址和端口绑定。\n",
    "#搜到这一行：c.NotebookApp.ip = 'LOCALHOST'\n",
    "#将LocalHost改为自己的机器名、域名或者IP地址。\n",
    "#不知道IP地址的，用ifconfig查看（linux上）如果设为\"*\"，则允许所有地址。\n",
    "\n",
    "#按照上面类似的方法，修改端口。\n",
    "\n",
    "#如果通过不同机器进行访问(无GUI的服务器，如云服务器必须设置)。\n",
    "#将browser允许一栏设为Disable，否则启动时可能导致挂起。\n",
    "\n",
    "#其它参数，根据需要设置。\n",
    "#保存，退出gedit。\n",
    "\n",
    "#启动Jupyter\n",
    "jupyter notebook\n",
    "\n",
    "#打开浏览器，远程访问\n",
    "http://myhost_ip:8888\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark分布式计算平台\n",
    "\n",
    "[Spark](http://spark.apache.org)是一个高性能分布式计算平台，通过基于内存的分布式数据对象RDD的使用，大幅度提高了MapReduce算法的性能，可以增强Hadoop等大数据系统的性能，扩展了流处理、拓扑分析、机器学习等分布式算法框架，同时能够兼容各种已有的存储系统。参见[更多介绍](http://my.oschina.net/u/2306127/blog/636184)。\n",
    "\n",
    "Spark通过PySpark模块支持基于Python的数据分析，同时也可以[与Jupyter Notebook一起使用](http://my.oschina.net/u/2306127/blog/636872)，创建优雅的数据探索工具和易于理解的数据分析教程，本文档的大部分教程亦使用该工具和方法创建。更多内容，请查看[Spark学习资源](http://www.oschina.net/search?q=Spark&scope=blog&onlyme=1)。\n",
    "\n",
    "### Spark大数据分析框架的组成\n",
    "\n",
    "[Spark](http://spark.apache.org)大数据分析框架的核心部件包含RDD内存数据结构、Streaming流计算框架、GraphX图计算与网状数据挖掘、MLlib机器学习支持框架、Spark SQL数据检索语言、Tachyon文件系统、SparkR计算引擎等主要部件。这里做一个简单的介绍。\n",
    "\n",
    "** (1)RDD内存数据结构 **\n",
    "\n",
    "大数据分析系统一般包括数据获取、数据清洗、数据处理、数据分析、报表输出等子系统。Spark为了方便数据处理、提升性能，专门引入了RDD数据内存结构，这一点与R的机制非常类似。用户程序只需要访问RDD的结构，与存储系统的数据调度、交换都由提供者驱动去实现。RDD可以与Haoop的HBase、HDFS等交互，用作数据存储系统，当然也可以通过扩展支持很多其它的数据存储系统。因为有了RDD，应用模型就与物理存储分离开来，而且能够更容易地处理大量数据记录遍历搜索的情况，这一点非常重要。因为Hadoop的结构主要适用于顺序处理，要翻回去反复检索数据的话效率就非常低下，而且缺乏一个统一的实现框架，由算法开发者自己去想办法实现。毫无疑问，这具有相当大的难度。RDD的出现，使这一问题得到了一定程度的解决。但正因为RDD是核心部件、实现难度大，这一块的性能、容量、稳定性直接决定着其它算法的实现程度。从目前看，还是经常会出现RDD占用的内存过载出问题的情况。\n",
    "\n",
    "** (2)Streaming流计算框架 **\n",
    "\n",
    "流是现在推特、微博、微信、图片服务以及物联网、位置服务等等的重要数据形态，因此流计算正显得前所未有的重要。流计算框架是所有互联网服务商的核心基础架构，Amazon、Microsoft都已经推出了Event消息总线云服务平台，而facebook\\twitter等更是将自己的流计算框架开源。Spark Streaming专门设计用于处理流式数据。通过Spark Streaming，可以快速地将数据推入处理环节，犹如流水线一样进行快速的加工，并在最短的时间反馈给使用。\n",
    "\n",
    "** (3)GraphX图计算与网状数据挖掘 **\n",
    "\n",
    "物理网络的拓扑结构，社交网络的连接关系，传统数据库的E-R关系，都是典型的图（Graph）数据模型。Hadoop主要适用于“数据量”很大的场合，对于关系的处理几乎没有支持，Hbase也是非常弱的关系处理能力。图数据结构往往需要快速多次对数据进行扫描式遍历，RDD的引入使Spark可以更高效地处理基于图的数据结构，从而使存储和处理大规模的图网络成为可能。类似的专用于图的系统还有neo4j等。GraphX相对于传统数据库的关系连接，可以处理更大规模、更深度的拓扑关系，可以在多个集群节点上进行运算，确实是现代数据关系研究的利器。\n",
    "\n",
    "** (4)MLlib机器学习支持框架 **\n",
    "\n",
    "通过把机器学习的算法移植到Spark架构上，一方面可以利用底层的大规模存储和RDD的数据快速访问能力，还可以利用图数据结构和集群计算的处理能力，使机器学习的运算可以在大规模的集群系统上展开，即大力拓展了机器学习算法的应用能力。\n",
    "\n",
    "** (5)Spark SQL数据检索语言 **\n",
    "\n",
    "这个跟基于Hive的实现有些类似，但是基于RDD理论上能提供更好的性能，同时能更方便处理如join和关系检索等操作。这个被设计为与用户交互的一个标准化入口。\n",
    "\n",
    "** (6)Tachyon文件系统 **\n",
    "\n",
    "Tachyon是基于内存的文件系统，可以使用HDFS等多种文件系统做持久化存储。已更名为Alluxio，通过将数据缓存到内存，从而大幅度提高了大尺寸文件的访问性能。\n",
    "\n",
    "** (7)SparkR计算引擎 ** \n",
    "\n",
    "将R语言的能力应用到Spark基础计算架构上，为其提供算法引擎。\n",
    "\n",
    "### Spark安装与部署\n",
    "\n",
    "安装Spark的工作，包括：安装Scala语言支持环境、安装SBT编译环境、安装Hadoop(可选)、按照需求配置Spark的运行模式。使用虚拟机、Docker来运行Spark或者直接使用Amazon/DataBricks的云端集群环境，可以大幅度简化安装和配置工作，快速建立自己的分布式计算集群。\n",
    "\n",
    "** Spark安装配置 **\n",
    "\n",
    "Spark的运行依赖于[Scala](http://www.scala-lang.org/)。首先[下载Scala](http://www.scala-lang.org/download/)，然后配置 scala 环境变量：\n",
    "\n",
    "```\n",
    "vim /etc/profile\n",
    "export SCALA_HOME=/home/hadoop/software/scala-2.11.4\n",
    "export PATH=$SCALA_HOME/bin:$PATH\n",
    "```\n",
    "\n",
    "从http://spark.apache.org/downloads.html 下载 [Spark](http://spark.apache.org)的版本，解压缩、[配置Spark环境变量](http://my.oschina.net/u/2306127/blog/550421)，运行即可。详细的安装方法参考[Spark 伪分布式 & 全分布式 安装指南](http://my.oschina.net/leejun2005/blog/394928)。\n",
    "\n",
    "** PySpark安装配置 **\n",
    "\n",
    "注意：如果要用pyspark，则需要设置 python 相关的 spark 包路径：\n",
    "\n",
    "```\n",
    "vi .bashrc\n",
    "export PYTHONPATH=$SPARK_HOME/python/:$SPARK_HOME/python/lib/py4j-0.8.2.1-src.zip:$PYTHONPATH\n",
    "\n",
    "```\n",
    "\n",
    "请根据你的机器环境，设置上面的路径的环境变量。\n",
    "\n",
    "** Spark源码编译 **\n",
    "\n",
    "Spark能同Hadoop进行交互，而Hadoop的厂商比较多有很多商业版。Spark官方提供的安装包不一定和我们的Hadoop集群版本相同，如果不相同就有可能出现莫名其妙的错误。这时，我们手工指定相应版本进行编译是最好选择，请参阅[详细参考](http://my.oschina.net/u/2306127/blog/546562)，详细的编译过程参阅[Spark官网指南](http://spark.apache.org/docs/latest/building-spark.html)。\n",
    "\n",
    "Spark编译有三种方式：SBT、MAVEN、make-distribution.sh。SBT、MAVEN两种方式打出来的包比较大，不适合部署使用，通常使用第三种方式打包。\n",
    "\n",
    "* 直接进Github，复制源码：https://github.com/apache/spark 。\n",
    "\n",
    "* SBT编译：`sbt/sbt clean assembly`\n",
    "\n",
    "* MAVEN编译。\n",
    "\n",
    "    * 由于MAVEN工具默认的内存比较小，需要先调大其占用的内存上限：  \n",
    "    `export MAVEN_OPTS=\"-Xmx2g -XX:MaxPermSize=512M -XX:ReservedCodeCacheSize=512m\"`\n",
    "\n",
    "    * 打包: `mvn clean assembly:assembly`\n",
    "\n",
    "* make-distribution.sh构建安装包。该脚本会使用MAVEN进行编译，然后打成一个tgz包。\n",
    "\n",
    "    * 脚本的使用方法：`./make-distribution.sh --help`\n",
    "    * 打包：`./make-distribution.sh --tgz --with-tachyon`\n",
    "\n",
    "\n",
    "### Spark的运行模式\n",
    "\n",
    "#### Spark本地运行模式\n",
    "\n",
    "控制台：pyspark --master local[4]，打开浏览器，访问 http://localhost:4040 可查看运行状态。\n",
    "\n",
    "PySpark缺省使用的Python环境是2.7，如果希望使用python3，可以设置环境变量 `PYSPARK_PYTHON=python3`。在控制台首先运行：`export PYSPARK_PYTHON=python3`，或者将上述语句加入pyspark的脚本中，或者加入~/.bashrc文件中，或者加到.profile。控制台窗口需要重启才有效。\n",
    "\n",
    "#### Spark集群运行模式\n",
    "\n",
    "这里介绍Spark的`Standalone`模式，即只有或主要运行Spark的集群。\n",
    "\n",
    "** A.启动Master **\n",
    "\n",
    "启动Master: start-master.sh，打开浏览器进入Master管理页面： http://localhost:8080 ，可以看到启动Master的监听URL在：\n",
    "\n",
    "        URL:          spark://supermap:7077\n",
    "        REST URL:     spark://supermap:6066(cluster mode)\n",
    "\n",
    "** B.启动Slave **\n",
    "\n",
    "启动Slave: start-slaves.sh --master spark://supermap:7077 \n",
    "        上面的--master参数为启动的master服务地址。\n",
    "        刷新Master的管理网页，可以看到新增加的worker。\n",
    "\n",
    "** C.启动控制台客户端 **\n",
    "\n",
    "连接集群: pyspark --master http://supermap:7077/\n",
    "\n",
    "        刷新Master的管理网页，可以看到新增加的Application.\n",
    "        点击PySparkShell，进去可以看到所运行的job等信息。\n",
    "        \n",
    "** D.提交批处理任务 **\n",
    "\n",
    "使用spark-submmit，具体参考：http://spark.apache.org/docs/latest/submitting-applications.html 。\n",
    "\n",
    "注意，提交任务既可以进入交互模式，也可以使用deploy-mode。交互模式下，客户端一直保持连接，并即时获得运行的信息。而deploy-mode提交后由后台运行，关闭客户端不影响任务的执行，运行信息需要通过日志文件去查看。\n",
    "\n",
    "#### Spark 协同运行模式\n",
    "\n",
    "如果在集群上会同时运行很多任务，Spark的调度器与别的调度器会竞争资源，就需要更高级的调度器来进行协调，一般使用mesos或YARN。这里不再详述，可参考：http://spark.apache.org/docs/latest/running-on-mesos.html 。\n",
    "\n",
    "Spark支持部署在虚拟机或者Docker上运行。\n",
    "\n",
    "### Spark 使用入门\n",
    "\n",
    "Spark支持多种方式调用，包括：开发Spark扩展组件、使用Scala控制台创建计算任务、使用PySpark进行交互式数据探索、SQL接口，通过Shell提交计算任务等等。Spark所提供的SQL接口以及DataFrame API，可以让拥护像传统的数据库一样进行操作大型分布式数据集，参见[Spark SQL和DataFrame指南](http://my.oschina.net/u/2306127/blog/683685)。\n",
    "\n",
    "Spark还在快速发展之中，Spark 2.0架构预计在2016年中推出，将带来更好的SQL支持、更好的性能和更好的软件生态。GISpark亦将为Spark注入先进的时空分析方法和地图可视化方法，拓展Spark的空间数据处理和分析能力。更多的Spark快速入门安装方法，访问[Spark 入门（Python、Scala 版）](http://my.oschina.net/u/2306127/blog/683680)，以及[Spark学习资源](http://my.oschina.net/u/2306127/blog/683687)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python 使用教程\n",
    "\n",
    "在大数据分析中，[Python](http://www.python.org)已经成为必不可少的工具。Python因为其简单、灵活、易用、兼容性好，具有非常丰富的各种各样的工具软件库，在数据探索、金融分析、科学研究、编程教学等方面得到了广泛的应用，Hadoop和Spark等都具有Python接口，大量的在线服务也都提供了Python API，可以方便地进行编程、实现自动化操作，直接进行数据获取和在线分析。\n",
    "\n",
    "Python([查看全部源码](https://github.com/python))完全由社区贡献的模式推动，发展非常迅速。参与社区和参加[PyCon大会](https://us.pycon.org/)、[PyData全球技术会议](http://pydata.org/)等活动都是了解最新的信息和提高编程和数据分析技能的捷径。欢迎查看[PyCon 2016 in Portland](https://us.pycon.org/2016/)的信息。\n",
    "\n",
    "### Python\n",
    "\n",
    "使用Python进行数据分析不需要专业的编程知识和长期的逻辑训练，可以通过这里的[Python基础教程](pystart/pystart_catalog.ipynb)快速学习。在[Notebook资源大全](pystart/git_list.ipynb)可以获得大量的从Python基础使用到数据处理、数据建模、并行计算、机器学习、深度学习等大量资源，采用notebook编写，也可以直接下载到本地进行练习，通过[下载Git资源到本地](pystart/git_get.md)可以快速将其下载到本地存储中慢慢学习。\n",
    "\n",
    "### Jupyter  \n",
    "\n",
    "[Jupyter](http://jupyter.org/)是一个优雅的Web编程环境，将Shell、Python、Ruby等多种脚本语言集成到一个执行环境中，可以混合在一起执行、设置，可以在多语言运行环境之间交换变量，使用[Jupyter魔法操作符](pystart/jupyter_magics.ipynb)即可。Jupyter可以通过Anaconda快速安装，参考[安装方法](http://my.oschina.net/u/2306127/blog/636289)，以及[阿里云服务器安装Jupyter及conda/Spark技巧](http://my.oschina.net/u/2306127/blog/657229)。\n",
    "\n",
    "Jupyter的核心组件是Notebook，源于[IPython项目](https://ipython.org/)(参见[IPython源码](https://github.com/ipython)，[IPython文档](https://ipython.org/documentation.html))，可以通过Web界面进行Python程序的编写，并可以转换为PDF等文档格式，可以产生漂亮的“可运行”文档，便于通过网络共享和协作式研究。详情参阅[Jupyter文档](http://jupyter.readthedocs.io/en/latest/index.html)。\n",
    "\n",
    "### NumPy和SciPy\n",
    "\n",
    "[NumPy](http://numpy.readthedocs.io/en/latest/)和[SciPy](http://scipy.org/)是著名的基于Python的科学计算软件库，提供\"大数变量\"、快捷矩阵运算和大量的科学函数库，在数学、物理、化学、生物等基础科学和医疗图像、基因研究、引力波探测、信号处理等科研活动中都得到了大量的应用，也是[Anaconda](http://continuum.io/)的基础软件包。\n",
    "\n",
    "### Pandas  \n",
    "\n",
    "数据分析常用的是[Python列表类对象](pystart_databasic.ipynb)，用于进行各种数据对象的处理。用于时间序列、二维表、数据立方分析的[Pandas内存数据表](pandas_quickstart.ipynb)功能非常强大，可以在内存中进行常规的数据库表的查询、修改、运算等操作。目前的[Pandas](http://pandas.pydata.org)主要是单机版本，在金融领域已经得到广泛的使用，一些社区正在开发可用于集群运算的Pandas版本，Spark的DataFrame以及提供了类似的分布式计算能力。 \n",
    "\n",
    "### Blaze  \n",
    "\n",
    "[Blaze系列软件](https://blaze.readthedocs.io/en/latest/)([源码](https://github.com/blaze))提供在集群上运行类似于NumPy和Pandas的功能。包括Blaze数据访问接口、[DataShape](http://datashape.pydata.org/)数据类型系统、[Odo](http://odo.pydata.org/)数据迁移系统、[DyND](https://github.com/libdynd/libdynd)内存动态数组、[Dask.array](http://dask.pydata.org/)多核/磁盘上NumPy arrays、[Dask.DataFrame](http://dask.pydata.org/)多核/磁盘上的Pandas DataFrame实现等子项目。 Blaze与NumPy和Pandas不同的是，Blaze直接将相应操作映射到多种数据存储系统，提供动态的数据访问能力。Blaze系列软件面向分布式的内存数据处理，目前仍然在快速的发展中。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python并行处理\n",
    "\n",
    "Python支持多种并行处理方法，包括MPI接口、ParallelPython、iPyParallel、Cuda、OpenCL以及PySpark(见本文下一节)。这里主要介绍[ipyparallel](    https://github.com/ipython/ipyparallel)，可以在Jupyter中直接启用，现在是IPython工程的一部分。\n",
    "\n",
    "** 安装ipyparallel **\n",
    "\n",
    "ipyparallel安装方法: `pip install ipyparallel`\n",
    "\n",
    "在Jupyter Notebook的IPython Clusters页面上启用ipyparallel: `ipcluster nbextension enable`\n",
    "\n",
    "关闭ipyparallel功能: `ipcluster nbextension disable`\n",
    "\n",
    "配置notebook server，查看 `jupyter_notebook_config.py` 相应的参数配置。\n",
    "\n",
    "** 运行ipyparallel **\n",
    "\n",
    "启动集群: `ipcluster start`\n",
    "\n",
    "在Python中使用:\n",
    "\n",
    "```\n",
    "import os\n",
    "import ipyparallel as ipp\n",
    "\n",
    "rc = ipp.Client()\n",
    "ar = rc[:].apply_async(os.getpid)\n",
    "pid_map = ar.get_dict()\n",
    "\n",
    "```\n",
    "\n",
    "更多的信息参考：https://ipyparallel.readthedocs.io/en/latest/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PySpark使用指南\n",
    "\n",
    "[PySpark](http://spark.apache.org/docs/latest/api/python/index.html)是Spark的客户端访问库，可以支持通过Python编写脚本，然后通过Spark在分布式环境中执行。使用方法参考 [快速体验Spark](pyspark/pyspark_quickstart.ipynb)，[PySpark教程](pyspark/pyspark_tutorial.ipynb)，[Spark快速入门](http://spark.apache.org/docs/latest/quick-start.html)，[Spark SQL和DataFrame使用](http://my.oschina.net/u/2306127/blog/683685)，[PySpark SQL API参考](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html)，[使用GraphFrames进行飞一般的图计算](http://my.oschina.net/u/2306127/blog/639268)。PySpark的安装和配置参考 [Spark 安装配置](http://my.oschina.net/u/2306127/blog/636184#OSC_h2_7)，[与Jupyter Notebook一起使用Spark](http://my.oschina.net/u/2306127/blog/636872)，[Spark配置参数](http://my.oschina.net/u/2306127/blog/639414)。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
